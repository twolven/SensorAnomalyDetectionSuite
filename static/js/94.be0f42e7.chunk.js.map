{"version":3,"file":"static/js/94.be0f42e7.chunk.js","mappings":"+DA+PA,QA5PA,MACIA,WAAAA,GAAe,KAiBfC,wBAA0B,KACtB,GAAyD,IAArDC,KAAKC,oBAAoBC,iBAAiBC,OAAc,OAAO,KAEnE,OADYH,KAAKC,oBAAoBC,iBAAiBE,QAAO,CAACC,EAAGC,IAAMD,EAAIC,GAAG,GACjEN,KAAKC,oBAAoBC,iBAAiBC,MAAM,EAnB7DH,KAAKO,QAAU,KACfP,KAAKQ,OAAS,KACdR,KAAKS,OAAS,GACdT,KAAKU,WAAa,IAClBV,KAAKW,QAAU,GACfX,KAAKY,aAAc,EACnBZ,KAAKa,QAAU,KACfb,KAAKc,qBAAuB,KAC5Bd,KAAKC,oBAAsB,CACvBc,cAAe,EACfC,eAAgB,EAChBC,eAAgB,EAChBf,iBAAkB,GAE1B,CAQA,gBAAMgB,GACF,IAEI,MAAMC,QAAsBC,MAAMC,6DAC5BC,QAAoBH,EAAcI,cAElCC,QAAuBJ,MAAMC,yDAuBnC,OAtBArB,KAAKQ,aAAegB,EAAeC,OAGnCzB,KAAKU,WAAaV,KAAKQ,OAAOkB,YAAY,GAC1C1B,KAAKa,QAAUb,KAAKQ,OAAOmB,cAAcC,kBACzC5B,KAAK6B,WAAa7B,KAAKQ,OAAOmB,cAAcG,YAG5C9B,KAAKO,cAAgBwB,EAAAA,GAAqBC,OAAOV,EAAa,CAC1DW,mBAAoB,CAAC,QACrBC,uBAAwB,QAG5BlC,KAAKY,aAAc,EACnBuB,QAAQC,IAAI,4CAA6C,CACrDC,WAAYrC,KAAKQ,OAAOkB,YACxBY,YAAatC,KAAKQ,OAAO+B,aACzBC,iBAAkBxC,KAAKa,QACvB4B,aAAczC,KAAKQ,OAAOmB,cAAce,cACxCC,mBAAoB3C,KAAKQ,OAAOmB,cAAciB,sBAG3C,CACHC,OAAQ,UACRrC,OAAQ,CACJsC,UAAW9C,KAAKU,WAChBG,QAASb,KAAKa,QACd4B,aAAczC,KAAKQ,OAAOmB,cAAce,eAGpD,CAAE,MAAOK,GAEL,OADAZ,QAAQY,MAAM,wBAAyBA,GAChC,CAAEF,OAAQ,QAASG,QAASD,EAAMC,QAC7C,CACJ,CAEAC,gBAAAA,CAAiBC,GACblD,KAAKc,qBAAuBoC,EAAY,CACpCC,KAAMD,EAAUC,KAChBC,UAAWF,EAAUE,UACrBC,UAAWC,KAAKC,MAChBC,UAAU,GACV,KAE8B,OAA9BxD,KAAKc,uBACLd,KAAKC,oBAAoBwD,kBAAoB,KAErD,CAEAC,cAAAA,CAAeC,GACX,GAAIA,EAAKxD,SAAWH,KAAKU,WACrB,MAAM,IAAIkD,MAAM,uBAADC,OAAwB7D,KAAKU,aAehD,OAXAyB,QAAQC,IAAI,kBAAmB0B,KAAKC,UAAU,CAC1CJ,KAAMK,MAAMC,KAAKN,GACjBO,MAAO,CACHC,IAAKC,KAAKD,OAAOR,GACjBU,IAAKD,KAAKC,OAAOV,GACjBW,KAAMX,EAAKvD,QAAO,CAACC,EAAGC,IAAMD,EAAIC,GAAG,GAAKqD,EAAKxD,OAC7CoE,QAASP,MAAMC,KAAKN,GAAMa,MAAM,EAAG,IACnCC,OAAQT,MAAMC,KAAKN,GAAMa,OAAO,QAIjC,IAAIzC,EAAAA,GACP,UACA,IAAI2C,aAAaf,GACjB,CAAC,EAAG3D,KAAKU,YAEjB,CAEA,eAAMiE,CAAUC,GACZ,IACI,MACMC,SADgB7E,KAAKO,QAAQuE,IAAI,CAAE,CAAC9E,KAAKO,QAAQwE,WAAW,IAAKH,KAChD5E,KAAKO,QAAQyE,YAAY,IAAIrB,KAC9CsB,EAAgBjB,MAAMC,KAAKY,GAGjC1C,QAAQC,IAAI,+BAAgC,CACxC8C,OAAQD,EAAc,GAAGE,QAAQ,GACjCC,IAAKH,EAAc,GAAGE,QAAQ,GAC9BE,MAAOJ,EAAc,GAAGE,QAAQ,GAChCG,SAAUL,EAAc,GAAGE,QAAQ,GACnCI,aAAcN,EAAc,GAAGE,QAAQ,KAG3C,MAAMK,EAAeP,EAAcQ,QAAQrB,KAAKC,OAAOY,IACjDS,EAAaT,EAAcO,GAGjC,OAAqB,IAAjBA,GAAsBE,EAAa,IAC5B,CACHC,MAAO,SACPV,cAAeA,EACfS,WAAYT,EAAc,GAC1BW,kBAAmB5F,KAAKQ,OAAOmB,cAAciB,oBAAoBiD,iBACjEC,YAAab,EAAc,GAAKjF,KAAKQ,OAAOmB,cAAciB,oBAAoBiD,kBAI/E,CACHF,MAAO3F,KAAKa,QAAQ2E,GACpBP,cAAeA,EACfS,WAAYA,EACZE,kBAAmB5F,KAAKQ,OAAOmB,cAAciB,oBAAoB,GAADiB,OAAI7D,KAAKa,QAAQ2E,GAAcO,cAAa,eAC5GD,YAAaJ,EAAa1F,KAAKQ,OAAOmB,cAAciB,oBAAoB,GAADiB,OAAI7D,KAAKa,QAAQ2E,GAAcO,cAAa,eAE3H,CAAE,MAAOhD,GAEL,MADAZ,QAAQY,MAAM,mBAAoBA,GAC5BA,CACV,CACJ,CAEAiD,qBAAAA,CAAsBC,GAClB,MAAM1C,EAAMD,KAAKC,MACX2C,GAAcC,EAAAA,EAAAA,GAAA,GAAQF,GAK5B,GAAIA,EAAOP,WAFkB,IAKzB,OAFAQ,EAAeP,MAAQ,SACvBO,EAAeR,WAAaO,EAAOP,WAC5BQ,EAGX,GAAIlG,KAAKc,qBAAsB,CAC3B,MAAMsF,EACFH,EAAON,MAAMI,gBAAkB/F,KAAKc,qBAAqBqC,KAAK4C,eAC9DE,EAAOP,YAXc,IAazB,GAAIU,IAAuBpG,KAAKc,qBAAqB0C,SAAU,CAC3DxD,KAAKc,qBAAqB0C,UAAW,EACrCxD,KAAKC,oBAAoBc,gBAEzB,MAAMsF,EAAU9C,EAAMvD,KAAKc,qBAAqBuC,UAChDrD,KAAKC,oBAAoBC,iBAAiBoG,KAAKD,GAC/CH,EAAehG,iBAAmBmG,EAElCH,EAAeK,oBAAsB,CACjCC,cAAexG,KAAKc,qBAAqBqC,KACzCsD,kBAAkB,EAClBrD,UAAWpD,KAAKc,qBAAqBsC,UACrCiD,UACAX,WAAYO,EAAOP,WAE3B,MAAYU,GAAuC,WAAjBH,EAAON,QACrC3F,KAAKC,oBAAoBe,iBACzBkF,EAAeK,oBAAsB,CACjCC,cAAexG,KAAKc,qBAAqBqC,KACzCsD,kBAAkB,EAClBC,WAAYT,EAAON,MACnBD,WAAYO,EAAOP,YAG/B,KAA0C,WAA/BO,EAAON,MAAMI,gBACpB/F,KAAKC,oBAAoBe,iBACzBkF,EAAeK,oBAAsB,CACjCC,cAAe,KACfC,kBAAkB,EAClBC,WAAYT,EAAON,MACnBD,WAAYO,EAAOP,aAW3B,OAPAQ,EAAeS,eAAiB,CAC5B5F,cAAef,KAAKC,oBAAoBc,cACxCC,eAAgBhB,KAAKC,oBAAoBe,eACzC4F,eAAgB5G,KAAKD,0BACrB2F,WAAYO,EAAOP,YAGhBQ,CACX,CAGA,mBAAMW,CAAclD,GAAiC,IAA3BmD,EAAiBC,UAAA5G,OAAA,QAAA6G,IAAAD,UAAA,GAAAA,UAAA,GAAG,KAC1C,IAAK/G,KAAKY,YACN,MAAM,IAAIgD,MAAM,sCAepB,GAZAzB,QAAQC,IAAI,oCAAqC,CAC7C6E,WAAYtD,EAAKxD,OACjB+G,eAAgBlH,KAAKU,WACrByG,mBAAoBL,IAIE,OAAtBA,GACA9G,KAAKiD,iBAAiB6D,GAItB9C,MAAMoD,QAAQzD,IAASA,EAAKxD,SAAWH,KAAKU,WAC5C,IACI,MAAMkE,EAAS5E,KAAK0D,eAAeC,GAC7BsC,QAAejG,KAAK2E,UAAUC,GACpC,OAAO5E,KAAKgG,sBAAsBC,EACtC,CAAE,MAAOlD,GAEL,MADAZ,QAAQY,MAAM,oBAAqBA,GAC7BA,CACV,CAGJ,OAAO,IACX,CAEAsE,kBAAAA,CAAmB1D,GACf,IAAI2D,EAAY,EAChB,IAAK,IAAIC,EAAI,EAAGA,EAAI5D,EAAKxD,OAAQoH,KACxB5D,EAAK4D,EAAE,GAAK,GAAK5D,EAAK4D,IAAM,GAC5B5D,EAAK4D,EAAE,IAAM,GAAK5D,EAAK4D,GAAK,IAC7BD,IAGR,OAAOA,CACX,GCzPEE,EAAMC,KACZ,IAAIC,EAAkB,KAElBC,EAAe,KAEnBH,EAAII,iBAAiB,WAAWC,eAAeC,GAC3C,MAAM,KAAE3E,EAAI,QAAE4E,GAAYD,EAAEnE,KAE5B,OAAQR,GACJ,IAAK,OACD,IACIhB,QAAQC,IAAI,+CACZsF,EAAkB,IAAIM,EACtB7F,QAAQC,IAAI,uCAEZ,MAAM6F,QAAmBP,EAAgBxG,aACzCiB,QAAQC,IAAI,yBAA0B6F,GAEZ,YAAtBA,EAAWpF,QACXV,QAAQC,IAAI,6CACZoF,EAAIU,YAAY,CACZ/E,KAAM,eACN4E,QAASE,MAGb9F,QAAQY,MAAM,yBAA0BkF,GACxCT,EAAIU,YAAY,CACZ/E,KAAM,aACN4E,QAASE,IAGrB,CAAE,MAAOlF,GACLZ,QAAQY,MAAM,+BAAgC,CAC1CC,QAASD,EAAMC,QACfmF,MAAOpF,EAAMoF,QAEjBX,EAAIU,YAAY,CACZ/E,KAAM,aACN4E,QAAS,CACL/E,QAASD,EAAMC,QACfmF,MAAOpF,EAAMoF,QAGzB,CACA,MAEA,IAAK,iBACoD,IAADC,EAAtD,IAAKV,IAAoBA,EAAgB9G,YASrC,OARAuB,QAAQC,IAAI,8BAA+B,CACvCiG,eAAgBX,EAChB9G,YAA4B,QAAjBwH,EAAEV,SAAe,IAAAU,OAAA,EAAfA,EAAiBxH,mBAElC4G,EAAIU,YAAY,CACZ/E,KAAM,QACN4E,QAAS,CAAE/E,QAAS,gCAK5B,IAEI,GAAI+E,EAAQO,eAAgB,CACxBnG,QAAQC,IAAI,4CAA6C,CACrDmG,aAAcR,EAAQO,eAAenI,OACrCqI,YAAaT,EAAQO,eAAe,GACpCG,WAAYV,EAAQO,eAAeP,EAAQO,eAAenI,OAAS,KAGvE,MAAM8F,QAAeyB,EAAgBb,cACjCkB,EAAQO,eACRP,EAAQvB,eAGZrE,QAAQC,IAAI,oBAAqB6D,GAE7BA,EACAuB,EAAIU,YAAY,CACZ/E,KAAM,SACN4E,QAAS,CACLW,UAAWzC,EAAON,MAClBD,WAAYO,EAAOP,WACnBI,YAAaG,EAAOH,YACpBb,cAAegB,EAAOhB,cACtBW,kBAAmBK,EAAOL,kBAC1BW,oBAAqBN,EAAOM,oBAC5BI,eAAgBV,EAAOU,kBAI/BxE,QAAQC,IAAI,kCAEpB,MACID,QAAQC,IAAI,0CAEpB,CAAE,MAAOW,GACLZ,QAAQY,MAAM,4BAA6B,CACvCC,QAASD,EAAMC,QACfmF,MAAOpF,EAAMoF,MACbJ,YAGJP,EAAIU,YAAY,CACZ/E,KAAM,QACN4E,QAAS,CACL/E,QAASD,EAAMC,QACf2F,QAAS5F,EAAMoF,QAG3B,CACA,MAEN,IAAK,qBACGT,IACAA,EAAgBzE,iBAAiB8E,EAAQ7E,WAEzCyE,EAAe,GACfH,EAAIU,YAAY,CACZ/E,KAAM,0BACN4E,QAAS,CAAE7E,UAAW6E,EAAQ7E,cAGtC,MAEJ,QACIf,QAAQyG,KAAK,wBAAyBzF,GAGlD,IAGAqE,EAAII,iBAAiB,SAAS,SAAS7E,GACnCZ,QAAQY,MAAM,uBAAwBA,GACtCyE,EAAIU,YAAY,CACZ/E,KAAM,QACN4E,QAAS,CACL/E,QAAS,wBAA0BD,EAAMC,QACzCmF,MAAOpF,EAAMoF,QAGzB,IAGAX,EAAII,iBAAiB,sBAAsB,SAASiB,GAChD1G,QAAQY,MAAM,+BAAgC8F,EAAMC,QACpDtB,EAAIU,YAAY,CACZ/E,KAAM,QACN4E,QAAS,CACL/E,QAAS,gCAAkC6F,EAAMC,OACjDX,MAAOU,EAAMC,OAAOX,QAGhC,G,GCzJIY,EAA2B,CAAC,EAGhC,SAASC,EAAoBC,GAE5B,IAAIC,EAAeH,EAAyBE,GAC5C,QAAqBjC,IAAjBkC,EACH,OAAOA,EAAaC,QAGrB,IAAIC,EAASL,EAAyBE,GAAY,CAGjDE,QAAS,CAAC,GAOX,OAHAE,EAAoBJ,GAAUG,EAAQA,EAAOD,QAASH,GAG/CI,EAAOD,OACf,CAGAH,EAAoBM,EAAID,EAGxBL,EAAoBO,EAAI,KAGvB,IAAIC,EAAsBR,EAAoBS,OAAEzC,EAAW,CAAC,MAAM,IAAOgC,EAAoB,MAE7F,OADAQ,EAAsBR,EAAoBS,EAAED,EAClB,E,MCjC3B,IAAIE,EAAW,GACfV,EAAoBS,EAAI,CAACxD,EAAQ0D,EAAUC,EAAIC,KAC9C,IAAGF,EAAH,CAMA,IAAIG,EAAeC,IACnB,IAASxC,EAAI,EAAGA,EAAImC,EAASvJ,OAAQoH,IAAK,CACrCoC,EAAWD,EAASnC,GAAG,GACvBqC,EAAKF,EAASnC,GAAG,GACjBsC,EAAWH,EAASnC,GAAG,GAE3B,IAJA,IAGIyC,GAAY,EACPC,EAAI,EAAGA,EAAIN,EAASxJ,OAAQ8J,MACpB,EAAXJ,GAAsBC,GAAgBD,IAAaK,OAAOC,KAAKnB,EAAoBS,GAAGW,OAAOC,GAASrB,EAAoBS,EAAEY,GAAKV,EAASM,MAC9IN,EAASW,OAAOL,IAAK,IAErBD,GAAY,EACTH,EAAWC,IAAcA,EAAeD,IAG7C,GAAGG,EAAW,CACbN,EAASY,OAAO/C,IAAK,GACrB,IAAIgD,EAAIX,SACE5C,IAANuD,IAAiBtE,EAASsE,EAC/B,CACD,CACA,OAAOtE,CArBP,CAJC4D,EAAWA,GAAY,EACvB,IAAI,IAAItC,EAAImC,EAASvJ,OAAQoH,EAAI,GAAKmC,EAASnC,EAAI,GAAG,GAAKsC,EAAUtC,IAAKmC,EAASnC,GAAKmC,EAASnC,EAAI,GACrGmC,EAASnC,GAAK,CAACoC,EAAUC,EAAIC,EAuBjB,C,KC3Bdb,EAAoBwB,EAAI,CAACrB,EAASsB,KACjC,IAAI,IAAIJ,KAAOI,EACXzB,EAAoB0B,EAAED,EAAYJ,KAASrB,EAAoB0B,EAAEvB,EAASkB,IAC5EH,OAAOS,eAAexB,EAASkB,EAAK,CAAEO,YAAY,EAAMC,IAAKJ,EAAWJ,IAE1E,ECNDrB,EAAoB8B,EAAI,CAAC,EAGzB9B,EAAoBlB,EAAKiD,GACjBC,QAAQC,IAAIf,OAAOC,KAAKnB,EAAoB8B,GAAG1K,QAAO,CAAC8K,EAAUb,KACvErB,EAAoB8B,EAAET,GAAKU,EAASG,GAC7BA,IACL,KCNJlC,EAAoBmC,EAAKJ,GAEjB,aAAeA,EAAf,qBCFR/B,EAAoBoC,SAAYL,IAEf,ECHjB/B,EAAoB0B,EAAI,CAACW,EAAKC,IAAUpB,OAAOqB,UAAUC,eAAeC,KAAKJ,EAAKC,GCAlFtC,EAAoB0C,EAAI,gC,MCAxB1C,EAAoB1I,EAAImH,KAAKkE,SAAW,aAIxC,IAAIC,EAAkB,CACrB,GAAI,GAkBL5C,EAAoB8B,EAAEvD,EAAI,CAACwD,EAASG,KAE/BU,EAAgBb,IAElBc,cAAc7C,EAAoB0C,EAAI1C,EAAoBmC,EAAEJ,GAE9D,EAGD,IAAIe,EAAqBrE,KAA2C,qCAAIA,KAA2C,sCAAK,GACpHsE,EAA6BD,EAAmBxF,KAAK0F,KAAKF,GAC9DA,EAAmBxF,KAzBC3C,IACnB,IAAIgG,EAAWhG,EAAK,GAChBsI,EAActI,EAAK,GACnBuI,EAAUvI,EAAK,GACnB,IAAI,IAAIsF,KAAYgD,EAChBjD,EAAoB0B,EAAEuB,EAAahD,KACrCD,EAAoBM,EAAEL,GAAYgD,EAAYhD,IAIhD,IADGiD,GAASA,EAAQlD,GACdW,EAASxJ,QACdyL,EAAgBjC,EAASwC,OAAS,EACnCJ,EAA2BpI,EAAK,C,WCrBjC,IAAIyI,EAAOpD,EAAoBO,EAC/BP,EAAoBO,EAAI,IAChBP,EAAoBlB,EAAE,KAAKuE,KAAKD,E,KCDdpD,EAAoBO,G","sources":["utils/PowerGridInference.js","workers/InferenceWorker.js","../webpack/bootstrap","../webpack/runtime/chunk loaded","../webpack/runtime/define property getters","../webpack/runtime/ensure chunk","../webpack/runtime/get javascript chunk filename","../webpack/runtime/get mini-css chunk filename","../webpack/runtime/hasOwnProperty shorthand","../webpack/runtime/publicPath","../webpack/runtime/importScripts chunk loading","../webpack/runtime/startup chunk dependencies","../webpack/startup"],"sourcesContent":["// src/utils/PowerGridInference.js\nimport * as ort from 'onnxruntime-web';\n\nclass PowerGridInference {\n    constructor() {\n        this.session = null;\n        this.config = null;\n        this.buffer = [];\n        this.bufferSize = 100;  // Explicitly set to match sequence_length\n        this.overlap = 50;      \n        this.initialized = false;\n        this.classes = null;\n        this.currentInjectedFault = null;\n        this.faultDetectionStats = {\n            truePositives: 0,\n            falsePositives: 0,\n            falseNegatives: 0,\n            detectionLatency: []\n        };\n    }\n\n    calculateAverageLatency = () => {\n        if (this.faultDetectionStats.detectionLatency.length === 0) return null;\n        const sum = this.faultDetectionStats.detectionLatency.reduce((a, b) => a + b, 0);\n        return sum / this.faultDetectionStats.detectionLatency.length;\n    };\n\n    async initialize() {\n        try {\n            // Load model and config\n            const modelResponse = await fetch(process.env.PUBLIC_URL + '/models/power_grid_model.onnx');\n            const modelBuffer = await modelResponse.arrayBuffer();\n            \n            const configResponse = await fetch(process.env.PUBLIC_URL + '/models/model_config.json');\n            this.config = await configResponse.json();\n\n            // Set class properties from config\n            this.bufferSize = this.config.input_shape[0];\n            this.classes = this.config.preprocessing.signal_conditions;\n            this.inputRange = this.config.preprocessing.input_range;\n            \n            // Initialize ONNX session\n            this.session = await ort.InferenceSession.create(modelBuffer, {\n                executionProviders: ['wasm'],\n                graphOptimizationLevel: 'all'\n            });\n\n            this.initialized = true;\n            console.log('Inference engine initialized with config:', {\n                inputShape: this.config.input_shape,\n                outputShape: this.config.output_shape,\n                signalConditions: this.classes,\n                samplingRate: this.config.preprocessing.sampling_rate,\n                performanceMetrics: this.config.preprocessing.performance_metrics\n            });\n\n            return { \n                status: 'success',\n                config: {\n                    inputSize: this.bufferSize,\n                    classes: this.classes,\n                    samplingRate: this.config.preprocessing.sampling_rate\n                }\n            };\n        } catch (error) {\n            console.error('Initialization error:', error);\n            return { status: 'error', message: error.message };\n        }\n    }\n\n    setInjectedFault(faultInfo) {\n        this.currentInjectedFault = faultInfo ? {\n            type: faultInfo.type,\n            magnitude: faultInfo.magnitude,\n            timestamp: Date.now(),\n            detected: false\n        } : null;\n\n        if (this.currentInjectedFault === null) {\n            this.faultDetectionStats.lastDetectionTime = null;\n        }\n    }\n\n    preprocessData(data) {\n        if (data.length !== this.bufferSize) {\n            throw new Error(`Data length must be ${this.bufferSize}`);\n        }\n    \n        // Save the data for analysis\n        console.log('React ML Input:', JSON.stringify({\n            data: Array.from(data),\n            stats: {\n                min: Math.min(...data),\n                max: Math.max(...data),\n                mean: data.reduce((a, b) => a + b, 0) / data.length,\n                first10: Array.from(data).slice(0, 10),\n                last10: Array.from(data).slice(-10)\n            }\n        }));\n    \n        return new ort.Tensor(\n            'float32',\n            new Float32Array(data),\n            [1, this.bufferSize]\n        );\n    }\n\n    async inference(tensor) {\n        try {\n            const results = await this.session.run({ [this.session.inputNames[0]]: tensor });\n            const output = results[this.session.outputNames[0]].data;\n            const probabilities = Array.from(output);\n            \n            // Log raw probabilities for each class\n            console.log('Raw inference probabilities:', {\n                normal: probabilities[0].toFixed(4),\n                sag: probabilities[1].toFixed(4),\n                swell: probabilities[2].toFixed(4),\n                harmonic: probabilities[3].toFixed(4),\n                interruption: probabilities[4].toFixed(4)\n            });\n    \n            const maxProbIndex = probabilities.indexOf(Math.max(...probabilities));\n            const confidence = probabilities[maxProbIndex];\n            \n            // Enforce stricter threshold for non-normal states\n            if (maxProbIndex !== 0 && confidence < 0.95) {  // If not normal and confidence < 95%\n                return {\n                    class: 'normal',\n                    probabilities: probabilities,\n                    confidence: probabilities[0],  // Use normal class probability\n                    baselinePrecision: this.config.preprocessing.performance_metrics.normal_precision,\n                    reliability: probabilities[0] * this.config.preprocessing.performance_metrics.normal_precision\n                };\n            }\n    \n            return {\n                class: this.classes[maxProbIndex],\n                probabilities: probabilities,\n                confidence: confidence,\n                baselinePrecision: this.config.preprocessing.performance_metrics[`${this.classes[maxProbIndex].toLowerCase()}_precision`],\n                reliability: confidence * this.config.preprocessing.performance_metrics[`${this.classes[maxProbIndex].toLowerCase()}_precision`]\n            };\n        } catch (error) {\n            console.error('Inference error:', error);\n            throw error;\n        }\n    }\n    \n    analyzeFaultDetection(result) {\n        const now = Date.now();\n        const enhancedResult = { ...result };\n    \n        // Only consider detections above threshold\n        const CONFIDENCE_THRESHOLD = 0.85;\n        \n        if (result.confidence < CONFIDENCE_THRESHOLD) {\n            enhancedResult.class = 'normal';\n            enhancedResult.confidence = result.confidence;\n            return enhancedResult;\n        }\n    \n        if (this.currentInjectedFault) {\n            const isCorrectDetection = \n                result.class.toLowerCase() === this.currentInjectedFault.type.toLowerCase() &&\n                result.confidence >= CONFIDENCE_THRESHOLD;\n    \n            if (isCorrectDetection && !this.currentInjectedFault.detected) {\n                this.currentInjectedFault.detected = true;\n                this.faultDetectionStats.truePositives++;\n                \n                const latency = now - this.currentInjectedFault.timestamp;\n                this.faultDetectionStats.detectionLatency.push(latency);\n                enhancedResult.detectionLatency = latency;\n    \n                enhancedResult.injectionValidation = {\n                    injectedFault: this.currentInjectedFault.type,\n                    detectionCorrect: true,\n                    magnitude: this.currentInjectedFault.magnitude,\n                    latency,\n                    confidence: result.confidence\n                };\n            } else if (!isCorrectDetection && result.class !== 'normal') {\n                this.faultDetectionStats.falsePositives++;\n                enhancedResult.injectionValidation = {\n                    injectedFault: this.currentInjectedFault.type,\n                    detectionCorrect: false,\n                    detectedAs: result.class,\n                    confidence: result.confidence\n                };\n            }\n        } else if (result.class.toLowerCase() !== 'normal') {\n            this.faultDetectionStats.falsePositives++;\n            enhancedResult.injectionValidation = {\n                injectedFault: null,\n                detectionCorrect: false,\n                detectedAs: result.class,\n                confidence: result.confidence\n            };\n        }\n    \n        enhancedResult.detectionStats = {\n            truePositives: this.faultDetectionStats.truePositives,\n            falsePositives: this.faultDetectionStats.falsePositives,\n            averageLatency: this.calculateAverageLatency(),\n            confidence: result.confidence\n        };\n    \n        return enhancedResult;\n    }\n\n    // In PowerGridInference.js, update the processSample method:\n    async processSample(data, injectedFaultInfo = null) {\n        if (!this.initialized) {\n            throw new Error('PowerGridInference not initialized');\n        }\n    \n        console.log('Processing in PowerGridInference:', {\n            dataLength: data.length,\n            expectedLength: this.bufferSize,\n            hasInjectedFault: !!injectedFaultInfo\n        });\n    \n        // Update fault info if changed\n        if (injectedFaultInfo !== null) {\n            this.setInjectedFault(injectedFaultInfo);\n        }\n    \n        // If we received a complete window, process it directly\n        if (Array.isArray(data) && data.length === this.bufferSize) {\n            try {\n                const tensor = this.preprocessData(data);\n                const result = await this.inference(tensor);\n                return this.analyzeFaultDetection(result);\n            } catch (error) {\n                console.error('Processing error:', error);\n                throw error;\n            }\n        }\n    \n        return null;\n    }\n\n    countZeroCrossings(data) {\n        let crossings = 0;\n        for (let i = 1; i < data.length; i++) {\n            if ((data[i-1] < 0 && data[i] >= 0) || \n                (data[i-1] >= 0 && data[i] < 0)) {\n                crossings++;\n            }\n        }\n        return crossings;\n    }\n}\n\nexport default PowerGridInference;","/* eslint-disable no-restricted-globals */\nimport PowerGridInference from '../utils/PowerGridInference';\n\nconst ctx = self;\nlet inferenceEngine = null;\n// Buffer for collecting samples before processing\nlet sampleBuffer = [];\n\nctx.addEventListener('message', async function(e) {\n    const { type, payload } = e.data;\n\n    switch (type) {\n        case 'INIT':\n            try {\n                console.log('Starting inference engine initialization...');\n                inferenceEngine = new PowerGridInference();\n                console.log('PowerGridInference instance created');\n                \n                const initResult = await inferenceEngine.initialize();\n                console.log('Initialization result:', initResult);\n                \n                if (initResult.status === 'success') {\n                    console.log('Inference engine successfully initialized');\n                    ctx.postMessage({ \n                        type: 'INIT_SUCCESS',\n                        payload: initResult \n                    });\n                } else {\n                    console.error('Initialization failed:', initResult);\n                    ctx.postMessage({ \n                        type: 'INIT_ERROR', \n                        payload: initResult \n                    });\n                }\n            } catch (error) {\n                console.error('Worker initialization error:', {\n                    message: error.message,\n                    stack: error.stack\n                });\n                ctx.postMessage({ \n                    type: 'INIT_ERROR', \n                    payload: { \n                        message: error.message,\n                        stack: error.stack\n                    } \n                });\n            }\n            break;\n\n            case 'PROCESS_SAMPLE':\n              if (!inferenceEngine || !inferenceEngine.initialized) {\n                  console.log('Inference engine not ready:', {\n                      engineExists: !!inferenceEngine,\n                      initialized: inferenceEngine?.initialized\n                  });\n                  ctx.postMessage({ \n                      type: 'ERROR', \n                      payload: { message: 'Inference engine not ready' } \n                  });\n                  return;\n              }\n          \n              try {\n                  // If we received a complete window, process it directly\n                  if (payload.completeWindow) {\n                      console.log('InferenceWorker received complete window:', {\n                          windowLength: payload.completeWindow.length,\n                          firstSample: payload.completeWindow[0],\n                          lastSample: payload.completeWindow[payload.completeWindow.length - 1]\n                      });\n                      \n                      const result = await inferenceEngine.processSample(\n                          payload.completeWindow,\n                          payload.injectedFault\n                      );\n                      \n                      console.log('Inference result:', result);\n                      \n                      if (result) {\n                          ctx.postMessage({ \n                              type: 'RESULT', \n                              payload: {\n                                  faultType: result.class,\n                                  confidence: result.confidence,\n                                  reliability: result.reliability,\n                                  probabilities: result.probabilities,\n                                  baselinePrecision: result.baselinePrecision,\n                                  injectionValidation: result.injectionValidation,\n                                  detectionStats: result.detectionStats\n                              }\n                          });\n                      } else {\n                          console.log('No result from inference engine');\n                      }\n                  } else {\n                      console.log('Received sample without complete window');\n                  }\n              } catch (error) {\n                  console.error('Detailed inference error:', {\n                      message: error.message,\n                      stack: error.stack,\n                      payload\n                  });\n                  \n                  ctx.postMessage({ \n                      type: 'ERROR', \n                      payload: { \n                          message: error.message,\n                          details: error.stack \n                      } \n                  });\n              }\n              break;\n              \n        case 'SET_INJECTED_FAULT':\n            if (inferenceEngine) {\n                inferenceEngine.setInjectedFault(payload.faultInfo);\n                // Clear buffer when fault changes\n                sampleBuffer = [];\n                ctx.postMessage({ \n                    type: 'FAULT_INJECTION_UPDATED',\n                    payload: { faultInfo: payload.faultInfo }\n                });\n            }\n            break;\n\n        default:\n            console.warn('Unknown message type:', type);\n            break;\n    }\n});\n\n// Add error handler for uncaught errors\nctx.addEventListener('error', function(error) {\n    console.error('Worker global error:', error);\n    ctx.postMessage({\n        type: 'ERROR',\n        payload: {\n            message: 'Worker global error: ' + error.message,\n            stack: error.stack\n        }\n    });\n});\n\n// Add unhandled rejection handler\nctx.addEventListener('unhandledrejection', function(event) {\n    console.error('Unhandled promise rejection:', event.reason);\n    ctx.postMessage({\n        type: 'ERROR',\n        payload: {\n            message: 'Unhandled promise rejection: ' + event.reason,\n            stack: event.reason.stack\n        }\n    });\n});\n\nexport {};","// The module cache\nvar __webpack_module_cache__ = {};\n\n// The require function\nfunction __webpack_require__(moduleId) {\n\t// Check if module is in cache\n\tvar cachedModule = __webpack_module_cache__[moduleId];\n\tif (cachedModule !== undefined) {\n\t\treturn cachedModule.exports;\n\t}\n\t// Create a new module (and put it into the cache)\n\tvar module = __webpack_module_cache__[moduleId] = {\n\t\t// no module.id needed\n\t\t// no module.loaded needed\n\t\texports: {}\n\t};\n\n\t// Execute the module function\n\t__webpack_modules__[moduleId](module, module.exports, __webpack_require__);\n\n\t// Return the exports of the module\n\treturn module.exports;\n}\n\n// expose the modules object (__webpack_modules__)\n__webpack_require__.m = __webpack_modules__;\n\n// the startup function\n__webpack_require__.x = () => {\n\t// Load entry module and return exports\n\t// This entry module depends on other loaded chunks and execution need to be delayed\n\tvar __webpack_exports__ = __webpack_require__.O(undefined, [284], () => (__webpack_require__(94)))\n\t__webpack_exports__ = __webpack_require__.O(__webpack_exports__);\n\treturn __webpack_exports__;\n};\n\n","var deferred = [];\n__webpack_require__.O = (result, chunkIds, fn, priority) => {\n\tif(chunkIds) {\n\t\tpriority = priority || 0;\n\t\tfor(var i = deferred.length; i > 0 && deferred[i - 1][2] > priority; i--) deferred[i] = deferred[i - 1];\n\t\tdeferred[i] = [chunkIds, fn, priority];\n\t\treturn;\n\t}\n\tvar notFulfilled = Infinity;\n\tfor (var i = 0; i < deferred.length; i++) {\n\t\tvar chunkIds = deferred[i][0];\n\t\tvar fn = deferred[i][1];\n\t\tvar priority = deferred[i][2];\n\t\tvar fulfilled = true;\n\t\tfor (var j = 0; j < chunkIds.length; j++) {\n\t\t\tif ((priority & 1 === 0 || notFulfilled >= priority) && Object.keys(__webpack_require__.O).every((key) => (__webpack_require__.O[key](chunkIds[j])))) {\n\t\t\t\tchunkIds.splice(j--, 1);\n\t\t\t} else {\n\t\t\t\tfulfilled = false;\n\t\t\t\tif(priority < notFulfilled) notFulfilled = priority;\n\t\t\t}\n\t\t}\n\t\tif(fulfilled) {\n\t\t\tdeferred.splice(i--, 1)\n\t\t\tvar r = fn();\n\t\t\tif (r !== undefined) result = r;\n\t\t}\n\t}\n\treturn result;\n};","// define getter functions for harmony exports\n__webpack_require__.d = (exports, definition) => {\n\tfor(var key in definition) {\n\t\tif(__webpack_require__.o(definition, key) && !__webpack_require__.o(exports, key)) {\n\t\t\tObject.defineProperty(exports, key, { enumerable: true, get: definition[key] });\n\t\t}\n\t}\n};","__webpack_require__.f = {};\n// This file contains only the entry chunk.\n// The chunk loading function for additional chunks\n__webpack_require__.e = (chunkId) => {\n\treturn Promise.all(Object.keys(__webpack_require__.f).reduce((promises, key) => {\n\t\t__webpack_require__.f[key](chunkId, promises);\n\t\treturn promises;\n\t}, []));\n};","// This function allow to reference async chunks and sibling chunks for the entrypoint\n__webpack_require__.u = (chunkId) => {\n\t// return url for filenames based on template\n\treturn \"static/js/\" + chunkId + \".\" + \"ab886646\" + \".chunk.js\";\n};","// This function allow to reference async chunks and sibling chunks for the entrypoint\n__webpack_require__.miniCssF = (chunkId) => {\n\t// return url for filenames based on template\n\treturn undefined;\n};","__webpack_require__.o = (obj, prop) => (Object.prototype.hasOwnProperty.call(obj, prop))","__webpack_require__.p = \"/SensorAnomalyDetectionSuite/\";","__webpack_require__.b = self.location + \"/../../../\";\n\n// object to store loaded chunks\n// \"1\" means \"already loaded\"\nvar installedChunks = {\n\t94: 1\n};\n\n// importScripts chunk loading\nvar installChunk = (data) => {\n\tvar chunkIds = data[0];\n\tvar moreModules = data[1];\n\tvar runtime = data[2];\n\tfor(var moduleId in moreModules) {\n\t\tif(__webpack_require__.o(moreModules, moduleId)) {\n\t\t\t__webpack_require__.m[moduleId] = moreModules[moduleId];\n\t\t}\n\t}\n\tif(runtime) runtime(__webpack_require__);\n\twhile(chunkIds.length)\n\t\tinstalledChunks[chunkIds.pop()] = 1;\n\tparentChunkLoadingFunction(data);\n};\n__webpack_require__.f.i = (chunkId, promises) => {\n\t// \"1\" is the signal for \"already loaded\"\n\tif(!installedChunks[chunkId]) {\n\t\tif(true) { // all chunks have JS\n\t\t\timportScripts(__webpack_require__.p + __webpack_require__.u(chunkId));\n\t\t}\n\t}\n};\n\nvar chunkLoadingGlobal = self[\"webpackChunksensor_anomaly_detection\"] = self[\"webpackChunksensor_anomaly_detection\"] || [];\nvar parentChunkLoadingFunction = chunkLoadingGlobal.push.bind(chunkLoadingGlobal);\nchunkLoadingGlobal.push = installChunk;\n\n// no HMR\n\n// no HMR manifest","var next = __webpack_require__.x;\n__webpack_require__.x = () => {\n\treturn __webpack_require__.e(284).then(next);\n};","// run startup\nvar __webpack_exports__ = __webpack_require__.x();\n"],"names":["constructor","calculateAverageLatency","this","faultDetectionStats","detectionLatency","length","reduce","a","b","session","config","buffer","bufferSize","overlap","initialized","classes","currentInjectedFault","truePositives","falsePositives","falseNegatives","initialize","modelResponse","fetch","process","modelBuffer","arrayBuffer","configResponse","json","input_shape","preprocessing","signal_conditions","inputRange","input_range","ort","create","executionProviders","graphOptimizationLevel","console","log","inputShape","outputShape","output_shape","signalConditions","samplingRate","sampling_rate","performanceMetrics","performance_metrics","status","inputSize","error","message","setInjectedFault","faultInfo","type","magnitude","timestamp","Date","now","detected","lastDetectionTime","preprocessData","data","Error","concat","JSON","stringify","Array","from","stats","min","Math","max","mean","first10","slice","last10","Float32Array","inference","tensor","output","run","inputNames","outputNames","probabilities","normal","toFixed","sag","swell","harmonic","interruption","maxProbIndex","indexOf","confidence","class","baselinePrecision","normal_precision","reliability","toLowerCase","analyzeFaultDetection","result","enhancedResult","_objectSpread","isCorrectDetection","latency","push","injectionValidation","injectedFault","detectionCorrect","detectedAs","detectionStats","averageLatency","processSample","injectedFaultInfo","arguments","undefined","dataLength","expectedLength","hasInjectedFault","isArray","countZeroCrossings","crossings","i","ctx","self","inferenceEngine","sampleBuffer","addEventListener","async","e","payload","PowerGridInference","initResult","postMessage","stack","_inferenceEngine","engineExists","completeWindow","windowLength","firstSample","lastSample","faultType","details","warn","event","reason","__webpack_module_cache__","__webpack_require__","moduleId","cachedModule","exports","module","__webpack_modules__","m","x","__webpack_exports__","O","deferred","chunkIds","fn","priority","notFulfilled","Infinity","fulfilled","j","Object","keys","every","key","splice","r","d","definition","o","defineProperty","enumerable","get","f","chunkId","Promise","all","promises","u","miniCssF","obj","prop","prototype","hasOwnProperty","call","p","location","installedChunks","importScripts","chunkLoadingGlobal","parentChunkLoadingFunction","bind","moreModules","runtime","pop","next","then"],"sourceRoot":""}